{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1GR4CBhwEVfzZHIOk6SZsCP6vFs0yjYWp",
      "authorship_tag": "ABX9TyMnuduQLjf5odeC7BKVN7O6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aknip/Modal/blob/main/vllm_llama-2-13b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VLLM_Llama-2-13b\n",
        "\n",
        "- REST API via FAST-API\n",
        "\n",
        "The app then can be automatically deployed to Modal.com.\n",
        "- Export Notebook to .py-file automatically (via nbdev)\n",
        "- Serve or deploy .py-file to Modal.com automatically\n",
        "\n",
        "Code blocks wich are needed for the final .py-file (for Modal.com) are marked with `#|export`\n",
        "\n",
        "Sources:  \n",
        "- https://github.com/jxnl/fastllm/blob/main/applications/vllm-struct/main.py\n"
      ],
      "metadata": {
        "id": "u7aFXzti5Okh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "IN_NOTEBOOK = any([\"jupyter-notebook\" in i for i in psutil.Process().parent().cmdline()])"
      ],
      "metadata": {
        "id": "HBQ9fzwbCTrr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uzJ0fadPQW5V"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "!pip install modal nbdev -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from getpass import getpass\n",
        "if IN_NOTEBOOK:\n",
        "  CREDS = json.loads(getpass(\"Secrets (JSON string): \"))\n",
        "  os.environ['CREDS'] = json.dumps(CREDS)\n",
        "  CREDS = json.loads(os.getenv('CREDS'))"
      ],
      "metadata": {
        "id": "D94_wG68CccK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8bd1faf-3aa9-4841-874c-494451f0abf9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Secrets (JSON string): ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"MODAL_TOKEN_ID\"] = CREDS['MODAL']['MODAL_TOKEN_ID']['credential']\n",
        "os.environ[\"MODAL_TOKEN_SECRET\"] = CREDS['MODAL']['MODAL_TOKEN_SECRET']['credential']"
      ],
      "metadata": {
        "id": "E9abYug6TRcR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The app - runs in Modal"
      ],
      "metadata": {
        "id": "yV-ta3j-6ZnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# uncomment following line to export\n",
        "# %%writefile vllm_llama-2-13b.py\n",
        "\n",
        "from modal import Stub, Image, Secret, method, asgi_app\n",
        "from typing import List\n",
        "import os\n",
        "import json\n",
        "\n",
        "import fastapi\n",
        "from pydantic import BaseModel\n",
        "\n",
        "app = fastapi.FastAPI(\n",
        "    title=\"vLLM\",\n",
        ")\n",
        "\n",
        "\n",
        "def download_model_to_folder():\n",
        "    from huggingface_hub import snapshot_download\n",
        "\n",
        "    #\"meta-llama/Llama-2-13b-chat-hf\",\n",
        "    snapshot_download(\n",
        "        \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "        local_dir=\"/model\",\n",
        "        token=os.environ[\"HUGGINGFACE_TOKEN\"],\n",
        "    )\n",
        "\n",
        "\n",
        "MODEL_DIR = \"/model\"\n",
        "\n",
        "image = (\n",
        "    Image.from_registry(\"nvcr.io/nvidia/pytorch:22.12-py3\")\n",
        "\n",
        "    .pip_install(\"torch==2.0.1\", index_url=\"https://download.pytorch.org/whl/cu118\")\n",
        "    #.pip_install(\"torch==2.0.1+cu118\", index_url=\"https://download.pytorch.org/whl/cu118\")\n",
        "\n",
        "    # NOT WORKING:\n",
        "    # Pin vLLM to 07/19/2023\n",
        "    #.pip_install(\n",
        "    #    \"vllm @ git+https://github.com/vllm-project/vllm.git@bda41c70ddb124134935a90a0d51304d2ac035e8\"\n",
        "    #)\n",
        "\n",
        "    # Pinned to 10/16/23\n",
        "    .pip_install(\n",
        "        \"vllm @ git+https://github.com/vllm-project/vllm.git@651c614aa43e497a2e2aab473493ba295201ab20\"\n",
        "    )\n",
        "    .pip_install(\"hf-transfer~=0.1\")\n",
        "    .env({\"HF_HUB_ENABLE_HF_TRANSFER\": \"1\"})\n",
        "    .run_function(download_model_to_folder, secret=Secret.from_name(\"huggingface\"))\n",
        ")\n",
        "\n",
        "stub = Stub(\"vllm\", image=image)\n",
        "\n",
        "\n",
        "# ## The model class\n",
        "#\n",
        "# The inference function is best represented with Modal's [class syntax](/docs/guide/lifecycle-functions) and the `__enter__` method.\n",
        "# This enables us to load the model into memory just once every time a container starts up, and keep it cached\n",
        "# on the GPU for each subsequent invocation of the function.\n",
        "#\n",
        "# The `vLLM` library allows the code to remain quite clean.\n",
        "@stub.cls(gpu=\"A100\", secret=Secret.from_name(\"huggingface\"))\n",
        "class Model:\n",
        "    def __enter__(self):\n",
        "        from vllm import LLM\n",
        "\n",
        "        # Load the model. Tip: MPT models may require `trust_remote_code=true`.\n",
        "        # We also add additional system prompting to the model to help it output json correctly.\n",
        "        self.llm = LLM(MODEL_DIR)\n",
        "        self.template = \"\"\"SYSTEM: Always correctly output response data as correctly formatted json in a codeblock\\n{system}\n",
        "USER: {input}\n",
        "ASSISTANT: ```json\\n\"\"\"\n",
        "\n",
        "    @method()\n",
        "    def generate(\n",
        "        self,\n",
        "        system: str,\n",
        "        inputs: List[str],\n",
        "        max_tokens: int = 800,\n",
        "        temperature: float = 0.1,\n",
        "        presence_penalty: float = 1.15,\n",
        "    ):\n",
        "        from vllm import SamplingParams\n",
        "\n",
        "        prompts = [self.template.format(system=system, input=ii) for ii in inputs]\n",
        "        sampling_params = SamplingParams(\n",
        "            temperature=temperature,\n",
        "            # we add a ``` to the end of the prompt to ensure the model outputs a codeblock\n",
        "            # improving the chances of it outputting correctly formatted json\n",
        "            stop=\"```\",\n",
        "            top_p=1,\n",
        "            max_tokens=max_tokens,\n",
        "            presence_penalty=presence_penalty,\n",
        "        )\n",
        "        result = self.llm.generate(prompts, sampling_params)\n",
        "        num_tokens = 0\n",
        "        results = [output.outputs[0].text for output in result]\n",
        "        num_tokens = sum([len(output.outputs[0].token_ids) for output in result])\n",
        "        return results, num_tokens\n",
        "\n",
        "\n",
        "class InputModel(BaseModel):\n",
        "    system: str\n",
        "    data: List[str]\n",
        "    max_tokens: int = 800\n",
        "    temperature: float = 0.1\n",
        "    presence_penalty: float = 1.15\n",
        "\n",
        "\n",
        "@app.post(\"/\")\n",
        "def main(input: InputModel):\n",
        "    def try_json(x):\n",
        "        try:\n",
        "            return json.loads(x)\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            return x\n",
        "\n",
        "    model = Model()\n",
        "    #data, num_tokens = model.generate.call(\n",
        "    data, num_tokens = model.generate.remote(\n",
        "        system=input.system,\n",
        "        inputs=input.data,\n",
        "        max_tokens=input.max_tokens,\n",
        "        temperature=input.temperature,\n",
        "        presence_penalty=input.presence_penalty,\n",
        "    )\n",
        "    return {\n",
        "        \"data\": [try_json(x) for x in data],\n",
        "        \"num_tokens\": num_tokens,\n",
        "    }\n",
        "\n",
        "\n",
        "@stub.function(image=image)\n",
        "@asgi_app()\n",
        "def fastapi_app():\n",
        "    return app"
      ],
      "metadata": {
        "id": "rDsRlP7m1-um",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b51e14ef-0a80-49df-bacb-fc8e11b7cdc7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting vllm_llama-2-13b.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run in Modal\n",
        "\n",
        "- might throws errors, eg. during some packages\n",
        "- solution: run again"
      ],
      "metadata": {
        "id": "nKaBR3-_7qWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Serve\n",
        "!modal serve vllm_llama-2-13b.py"
      ],
      "metadata": {
        "id": "1X3slh4n3xGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check URL in Browser\n",
        "https://aknip--vllm-fastapi-app-dev.modal.run/docs\n",
        "\n",
        "# Send POST via Terminal\n",
        "```\n",
        "curl -X 'POST' \\\n",
        "  'https://aknip--vllm-fastapi-app-dev.modal.run' \\\n",
        "  -H 'accept: application/json' \\\n",
        "  -H 'Content-Type: application/json' \\\n",
        "  -d '{\n",
        "   \"system\":\"Extract Users from `Interface Users { users: Array<{name: string, age:number}>}`\",\n",
        "   \"data\":[\n",
        "      \"James, 33, and Isabella, 23, are among the users with Benjamin, who is 34, Mia, 30, and Ethan, 28.\",\n",
        "      \"Evelyn, 25, and Jacob, 29, are part of the records, along with Abigail, 27, Liam, 32, and Harper, 26.\"\n",
        "   ]\n",
        "}'\n",
        "``"
      ],
      "metadata": {
        "id": "1xwHOXPutav1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Local run\n",
        "# 1st run / initial run:\n",
        "# 2nd run:\n",
        "# 3rd run:\n",
        "!modal run vllm_llama-2-13b.py"
      ],
      "metadata": {
        "id": "4d-Kxt51IJXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Deploy server permanently\n",
        "!modal deploy vllm_llama-2-13b.py"
      ],
      "metadata": {
        "id": "fn2ufbIe47vc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}