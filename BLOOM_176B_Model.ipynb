{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1GR4CBhwEVfzZHIOk6SZsCP6vFs0yjYWp",
      "authorship_tag": "ABX9TyO0hIP9pbANcRags43u/nrS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aknip/Modal/blob/main/BLOOM_176B_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BLOOM 176B Model\n",
        "\n",
        "BLOOM Chat 176B GPTQ example. It is the only open-source model, whose size matches the model behind ChatGPT. It requires 5 GPUs of A100 40GB to run load. Possibly more for higher context window. Cold start takes 90-100 seconds. Generation takes around 30 seconds.\n",
        "\n",
        "This notebook shows how to develop a Gradio app in a notebook.\n",
        "The app then can be automatically deployed to Modal.com.\n",
        "- Export Notebook to .py-file automatically (via nbdev)\n",
        "- Serve or deploy .py-file to Modal.com automatically\n",
        "\n",
        "Code blocks wich are needed for the final .py-file (for Modal.com) are marked with `#|export`\n",
        "\n",
        "Sources:  \n",
        "- https://github.com/modal-labs/modal-examples/pull/357\n",
        "- Code: https://github.com/modal-labs/modal-examples/blob/73ebb35c220b59c2aa29a4e827b3d171f49151a6/06_gpu_and_ml/bloom_gptq.py\n"
      ],
      "metadata": {
        "id": "u7aFXzti5Okh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "IN_NOTEBOOK = any([\"jupyter-notebook\" in i for i in psutil.Process().parent().cmdline()])"
      ],
      "metadata": {
        "id": "HBQ9fzwbCTrr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uzJ0fadPQW5V",
        "outputId": "c19b55d7-2c95-45af-a11a-dd54a942cbfd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.1/330.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.1/66.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.7/58.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for grpclib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install modal nbdev -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from getpass import getpass\n",
        "if IN_NOTEBOOK:\n",
        "  CREDS = json.loads(getpass(\"Secrets (JSON string): \"))\n",
        "  os.environ['CREDS'] = json.dumps(CREDS)\n",
        "  CREDS = json.loads(os.getenv('CREDS'))"
      ],
      "metadata": {
        "id": "D94_wG68CccK",
        "outputId": "80e3f5ad-22f6-4428-ef07-e835e549933e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Secrets (JSON string): ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"MODAL_TOKEN_ID\"] = CREDS['MODAL']['MODAL_TOKEN_ID']['credential']\n",
        "os.environ[\"MODAL_TOKEN_SECRET\"] = CREDS['MODAL']['MODAL_TOKEN_SECRET']['credential']"
      ],
      "metadata": {
        "id": "E9abYug6TRcR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The app - runs in Modal"
      ],
      "metadata": {
        "id": "yV-ta3j-6ZnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# uncomment following line to export\n",
        "# %%writefile bloom_176b_model.py\n",
        "\n",
        "#|export\n",
        "\n",
        "# ---\n",
        "# integration-test: false\n",
        "# ---\n",
        "# # Run Bloom Chat 176B model (ChatGPT-size) with AutoGPTQ\n",
        "\n",
        "# In this example, we run a quantized 4-bit version of Sambanova Systems's BLOOMChat 1.0, the only open-source large language\n",
        "# model of the size matching GPT 3.5 model used in the well-known ChatGPT, using HuggingFace's [transformers](https://huggingface.co/docs/transformers/index) library and [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ).\n",
        "#\n",
        "# We use the quantized version of the model created by TheBloke [BLOOMChat-176B-v1-GPTQ](https://huggingface.co/TheBloke/BLOOMChat-176B-v1-GPTQ).\n",
        "# It is a great idea to checkout his Model Card (`README.md`) for more details on the model.\n",
        "# It is also possible to run the original BLOOM model as well [bloomz-176B-GPTQ](https://huggingface.co/TheBloke/bloomz-176B-GPTQ).\n",
        "#\n",
        "# Due its enormous size the model files are around 100GBs and they take a while to download and set up. Occasionally there may be a network error.\n",
        "# Please kindly retry the process. The build shouldn't take more than 20 minutes.\n",
        "# This example includes verbose feedback to help you track the progress.\n",
        "#\n",
        "# Cold boot time on 5x A100 40GB GPUs is around 90s.\n",
        "#\n",
        "# ## Setup\n",
        "#\n",
        "# First we import the components we need from `modal`. We also import `time` utility to provide us with insights into the performance.\n",
        "\n",
        "import time\n",
        "from modal import Image, method, Stub, web_endpoint, gpu\n",
        "\n",
        "stub = Stub(name=\"example-bloom-gptq\") # change this to your own name\n",
        "\n",
        "IMAGE_MODEL_DIR = \"/model\"\n",
        "MODEL_BASE_FILE = \"gptq_model-4bit--1g\" # the model file name without the \".safetensors\" suffix\n",
        "\n",
        "# Bloom models were split to pass Hugging Face's 50GB limit. Therefore after downloading we will merge the model files into a single file.\n",
        "SPLIT_FILE_REGEX = \"gptq_model-4bit--1g.JOINBEFOREUSE.split-*.safetensors\"\n",
        "command = f\"cd {IMAGE_MODEL_DIR} && cat {SPLIT_FILE_REGEX} > {MODEL_BASE_FILE}.safetensors && rm {SPLIT_FILE_REGEX}\"\n",
        "\n",
        "\n",
        "# Here we declare a function to download the model during the build time.\n",
        "def download_model():\n",
        "    import transformers\n",
        "    from huggingface_hub import snapshot_download\n",
        "    MODEL_NAME = \"TheBloke/BLOOMChat-176B-v1-GPTQ\"\n",
        "\n",
        "    # Verify at least 200GB is available.\n",
        "    # This is to ensure that the model can be downloaded and merged.\n",
        "    # The merged file will be deleted after the model is loaded.\n",
        "    import shutil\n",
        "    total, used, free = shutil.disk_usage(\"/\")\n",
        "    assert free > 200 * 1024 * 1024 * 1024, f\"Expected at least 200GB free space. Got {free}\"\n",
        "\n",
        "    # The download may fail once in a while. Simply rerun the script again.\n",
        "    print(f\"Downloading model... expect 3-15 minutes...\")\n",
        "    start_time = time.time()\n",
        "    snapshot_download(MODEL_NAME,\n",
        "        local_dir=IMAGE_MODEL_DIR,\n",
        "        resume_download=True,\n",
        "        # token is optional but it will speed up the download\n",
        "        # token=\"hf_xxx\"\n",
        "    )\n",
        "    end_time = time.time()\n",
        "    print(f\"Download completed, took => {end_time - start_time:.2f}s\")\n",
        "\n",
        "    print(\"Combining model files... expect 3 minutes...\")\n",
        "    import subprocess\n",
        "    subprocess.run(command, check=True, shell=True)\n",
        "    print(f\"Model files combined, took => {time.time() - end_time:.2f}s\")\n",
        "    end_time = time.time()\n",
        "\n",
        "    # We move cache to avoid doing that during inference time.\n",
        "    print(\"Moving cache... expect 2-4 minutes...\")\n",
        "    transformers.utils.move_cache()\n",
        "    print(f\"Cache moved, took => {time.time() - end_time:.2f}s\")\n",
        "\n",
        "    print(\"Done! Modal may take up to 15 minutes to upload a snapshot...\")\n",
        "\n",
        "\n",
        "inference_image = (\n",
        "    Image.from_dockerhub(\n",
        "        \"nvidia/cuda:11.8.0-devel-ubuntu22.04\",\n",
        "        setup_dockerfile_commands=[\n",
        "            \"RUN apt-get update\",\n",
        "            \"RUN apt-get install -y python3 python3-pip python-is-python3\",\n",
        "        ],\n",
        "    )\n",
        "    .apt_install(\"git\", \"gcc\", \"build-essential\")\n",
        "    .run_commands(\n",
        "        \"pip install --compile huggingface_hub transformers torch einops hf_transfer\",\n",
        "    )\n",
        "    .env({\n",
        "            \"HF_HUB_ENABLE_HF_TRANSFER\": \"1\", # enable fast downloads, this mediates common Hugging Face Read Timeouts\n",
        "            \"PIP_NO_CACHE_DIR\": \"1\",\n",
        "            \"PIP_DISABLE_PIP_VERSION_CHECK\": \"1\",\n",
        "            \"SAFETENSORS_FAST_GPU\": \"1\", # Load the model directly to GPU memory skipping RAM\n",
        "            \"BITSANDBYTES_NOWELCOME\": \"1\",\n",
        "        })\n",
        "    .run_function(download_model)\n",
        "    .run_commands(\n",
        "        # It appears that installing directly through pip torch extension fails to compile.\n",
        "        # As such, we clone it and install it from source whilst providing T4 GPU.\n",
        "        \"git clone https://github.com/PanQiWei/AutoGPTQ.git\",\n",
        "        \"cd AutoGPTQ && pip install --compile .\",\n",
        "        gpu=\"T4\",\n",
        "    )\n",
        ")\n",
        "\n",
        "api_image = (\n",
        "    Image.debian_slim()\n",
        ")\n",
        "\n",
        "\n",
        "@stub.cls(image=inference_image, gpu=gpu.A100(count=5), container_idle_timeout=300, cloud=\"oci\", concurrency_limit=1)\n",
        "class BloomChat:\n",
        "    def __enter__(self):\n",
        "        start_import = time.time()\n",
        "        import torch\n",
        "        from transformers import AutoTokenizer, StoppingCriteria, StoppingCriteriaList\n",
        "        from auto_gptq import AutoGPTQForCausalLM\n",
        "\n",
        "        print(f\"importing libraries took => {time.time() - start_import:.2f}s\")\n",
        "\n",
        "        start_load_tokenizer = time.time()\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            IMAGE_MODEL_DIR, use_fast=True\n",
        "        )\n",
        "        print(f\"loading tokenizer took => {time.time() - start_load_tokenizer:.2f}s\")\n",
        "\n",
        "        start_loading_model = time.time()\n",
        "        print(\"loading model...\")\n",
        "\n",
        "        self.model = AutoGPTQForCausalLM.from_quantized(\n",
        "            IMAGE_MODEL_DIR,\n",
        "            model_basename=MODEL_BASE_FILE,\n",
        "            use_safetensors=True,\n",
        "            device_map=\"auto\",\n",
        "            use_triton=False,\n",
        "            strict=True,\n",
        "        )\n",
        "        self.model.tie_weights()\n",
        "\n",
        "        print(f\"Model loaded in =>  {time.time() - start_loading_model:.2f}s\")\n",
        "\n",
        "        cold_boot_time = time.time() - start_import\n",
        "        print(f\"total cold boot time  => {cold_boot_time:.2f}s\")\n",
        "\n",
        "        self.is_loaded = False\n",
        "        self.cold_boot_time = cold_boot_time\n",
        "\n",
        "    @method()\n",
        "    async def generate(self, input, temperature = 0.7, max_tokens = 256, stop_words = [\"\"]):\n",
        "        import torch\n",
        "        from transformers import StoppingCriteria, StoppingCriteriaList\n",
        "\n",
        "        # Cold boot time, should be zero if model is already loaded\n",
        "        if (self.is_loaded == False):\n",
        "            cold_boot_time = self.cold_boot_time\n",
        "            self.is_loaded = True\n",
        "        else:\n",
        "            cold_boot_time = 0\n",
        "\n",
        "        stop_token_ids = self.tokenizer.convert_tokens_to_ids(stop_words)\n",
        "        class StopOnTokens(StoppingCriteria):\n",
        "            def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "                for stop_id in stop_token_ids:\n",
        "                    if input_ids[0][-1] == stop_id:\n",
        "                        return True\n",
        "                return False\n",
        "        stopping_criteria = StoppingCriteriaList([StopOnTokens()])\n",
        "\n",
        "        t3 = time.time()\n",
        "        input_ids = self.tokenizer(input, return_tensors='pt').input_ids.cuda()\n",
        "        input_tokens= len(input_ids[0])\n",
        "        generation = self.model.generate(inputs=input_ids,\n",
        "            temperature=temperature,\n",
        "            do_sample=True if temperature > 0 else False,\n",
        "            max_new_tokens=max_tokens,\n",
        "            repetition_penalty=1.1,\n",
        "            stopping_criteria=stopping_criteria if len(stop_words) > 0 else None,\n",
        "        )\n",
        "        completion_tokens = len(generation[0]) - input_tokens\n",
        "\n",
        "        # Provide completion without the prompt\n",
        "        # Subtract the input tokens from the generated tokens\n",
        "        new_tokens = generation[0][input_tokens:]\n",
        "\n",
        "        completion = self.tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
        "        latency = time.time() - t3\n",
        "\n",
        "        print(f\"Input tokens: {input_tokens}\")\n",
        "        print(f\"Completion tokens: {completion_tokens}\")\n",
        "        print(f\"Generation took => {latency:.2f}s\")\n",
        "\n",
        "        return {\n",
        "            \"completion\": completion,\n",
        "            \"completion_tokens\": completion_tokens,\n",
        "            \"prompt_tokens\": input_tokens,\n",
        "            \"execution_time\": latency,\n",
        "            \"delay_time\": cold_boot_time,\n",
        "            \"model\": stub.name,\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "DEMO_INPUT = \"\"\"\n",
        "<human>: What is Modal?\n",
        "<bot>: Modal (modal.com) lets you run code in the cloud without having to think about infrastructure.\n",
        "Features\n",
        "- Run any code remotely within seconds.\n",
        "- Define container environments in code (or use one of our pre-built backends).\n",
        "- Scale up horizontally to thousands of containers.\n",
        "- Deploy and monitor persistent cron jobs.\n",
        "- Attach GPUs with a single line of code.\n",
        "- Serve your functions as web endpoints.\n",
        "- Use powerful primitives like distributed dictionaries and queues.\n",
        "- Run your code on a schedule.\n",
        "<human>: What is the future of Modal?\n",
        "<bot>:\n",
        "\"\"\"\n",
        "\n",
        "@stub.local_entrypoint()\n",
        "def main():\n",
        "    t0 = time.time()\n",
        "    model = BloomChat()\n",
        "    val= model.generate.call(DEMO_INPUT)\n",
        "    print(val)\n",
        "    print(f\"Total time: {time.time() - t0:.2f}s\")\n",
        "\n",
        "\n",
        "from pydantic import BaseModel\n",
        "from typing_extensions import Annotated\n",
        "from typing import List, Union\n",
        "\n",
        "class CompletionRequest(BaseModel):\n",
        "    prompt: Annotated[str, \"The prompt for text completion\"]\n",
        "    temperature: Annotated[\n",
        "        float,\n",
        "        \"Adjusts randomness of outputs, greater than 1 is random and 0 is deterministic.\",\n",
        "    ] = 0.7\n",
        "    max_tokens: Annotated[\n",
        "        int, \"Maximum number of new tokens to generate for text completion.\"\n",
        "    ] = 16\n",
        "    stop_words: Annotated[Union[str, List[str]], \"Any additional stop words.\"] = []\n",
        "    ref: Annotated[str, \"Reference string for the completion\"] = \"\"\n",
        "\n",
        "\n",
        "@stub.function(image=api_image, cloud=\"oci\", concurrency_limit=1)\n",
        "@web_endpoint(method=\"POST\")\n",
        "def api(request: CompletionRequest):\n",
        "    t = time.time()\n",
        "    print(f\"Request received: {request.ref}\")\n",
        "    result = BloomChat().generate.call(input=request.prompt,\n",
        "                temperature=request.temperature,\n",
        "                max_tokens=request.max_tokens,\n",
        "                stop_words=request.stop_words\n",
        "            )\n",
        "\n",
        "    result[\"ref\"] = request.ref\n",
        "\n",
        "    print(f\"Request completed: {request.ref} => {time.time() - t:.2f}s\")\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "rDsRlP7m1-um",
        "outputId": "f3f1e9c7-8503-47ab-995c-76125f4b2ced",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing bloom_176b_model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Export production file for Modal with nbdev\n",
        "\n",
        "The notebook file will be copied from **Google drive** to the current notebook environment.\n",
        "\n",
        "Alternative?: %notebook modal_hello_world2.ipynb"
      ],
      "metadata": {
        "id": "NRQoSKU51v6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect Google Drive\n",
        "# This and the following cell can be skipped it the notebook file is copied manually to the current notebook environment\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "RI3oYbrHwxtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy Notebook to local path\n",
        "!cp \"/content/drive/MyDrive/Colab Notebooks/BLOOM_176B_Model.ipynb\" /content/"
      ],
      "metadata": {
        "id": "KF3b5UaVyi8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export source code marked with #|export\n",
        "from nbdev.export import nb_export\n",
        "nb_export('/content/BLOOM_176B_Model.ipynb', lib_path='.', name='bloom_176b_model')"
      ],
      "metadata": {
        "id": "TqPqsbyv7X5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run in Modal"
      ],
      "metadata": {
        "id": "nKaBR3-_7qWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dev\n",
        "!modal serve bloom_176b_model.py"
      ],
      "metadata": {
        "id": "1X3slh4n3xGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Deploy server permanently\n",
        "!modal deploy bloom_176b_model.py"
      ],
      "metadata": {
        "id": "fn2ufbIe47vc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}