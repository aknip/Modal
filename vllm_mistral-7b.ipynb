{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1GR4CBhwEVfzZHIOk6SZsCP6vFs0yjYWp",
      "authorship_tag": "ABX9TyM8ADsikafe+BqPsEyncfE5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aknip/Modal/blob/main/vllm_mistral-7b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VLLM Mistral 7B Model\n",
        "\n",
        "2 Examples:\n",
        "- Run as API\n",
        "- Run as app / command line\n",
        "\n",
        "The app then can be automatically deployed to Modal.com.\n",
        "- Export Notebook to .py-file automatically (via nbdev)\n",
        "- Serve or deploy .py-file to Modal.com automatically\n",
        "\n",
        "Code blocks wich are needed for the final .py-file (for Modal.com) are marked with `#|export`\n",
        "\n",
        "Sources:  \n",
        "- https://github.com/modal-labs/modal-examples/blob/main/06_gpu_and_ml/vllm_inference.py\n",
        "- for API: https://github.com/jxnl/fastllm/blob/main/applications/vllm-struct/main.py"
      ],
      "metadata": {
        "id": "u7aFXzti5Okh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "IN_NOTEBOOK = any([\"jupyter-notebook\" in i for i in psutil.Process().parent().cmdline()])"
      ],
      "metadata": {
        "id": "HBQ9fzwbCTrr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uzJ0fadPQW5V"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "!pip install modal nbdev -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from getpass import getpass\n",
        "if IN_NOTEBOOK:\n",
        "  CREDS = json.loads(getpass(\"Secrets (JSON string): \"))\n",
        "  os.environ['CREDS'] = json.dumps(CREDS)\n",
        "  CREDS = json.loads(os.getenv('CREDS'))"
      ],
      "metadata": {
        "id": "D94_wG68CccK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dc3d411-35ce-4938-cec8-eb18a5a3d61b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Secrets (JSON string): ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"MODAL_TOKEN_ID\"] = CREDS['MODAL']['MODAL_TOKEN_ID']['credential']\n",
        "os.environ[\"MODAL_TOKEN_SECRET\"] = CREDS['MODAL']['MODAL_TOKEN_SECRET']['credential']"
      ],
      "metadata": {
        "id": "E9abYug6TRcR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The API - runs in Modal"
      ],
      "metadata": {
        "id": "-lUIi8ZwuK9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# uncomment following line to export\n",
        "# %%writefile vllm_mistral-7b-api.py\n",
        "\n",
        "from modal import Stub, Image, Secret, method, asgi_app\n",
        "from typing import List\n",
        "import os\n",
        "import json\n",
        "\n",
        "import fastapi\n",
        "from pydantic import BaseModel\n",
        "\n",
        "app = fastapi.FastAPI(\n",
        "    title=\"vLLM\",\n",
        ")\n",
        "\n",
        "\n",
        "def download_model_to_folder():\n",
        "    from huggingface_hub import snapshot_download\n",
        "\n",
        "    #\"meta-llama/Llama-2-13b-chat-hf\",\n",
        "    snapshot_download(\n",
        "        \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "        local_dir=\"/model\",\n",
        "        token=os.environ[\"HUGGINGFACE_TOKEN\"],\n",
        "    )\n",
        "\n",
        "\n",
        "MODEL_DIR = \"/model\"\n",
        "\n",
        "image = (\n",
        "    Image.from_registry(\"nvcr.io/nvidia/pytorch:22.12-py3\")\n",
        "\n",
        "    .pip_install(\"torch==2.0.1\", index_url=\"https://download.pytorch.org/whl/cu118\")\n",
        "    #.pip_install(\"torch==2.0.1+cu118\", index_url=\"https://download.pytorch.org/whl/cu118\")\n",
        "\n",
        "    # NOT WORKING:\n",
        "    # Pin vLLM to 07/19/2023\n",
        "    #.pip_install(\n",
        "    #    \"vllm @ git+https://github.com/vllm-project/vllm.git@bda41c70ddb124134935a90a0d51304d2ac035e8\"\n",
        "    #)\n",
        "\n",
        "    # Pinned to 10/16/23\n",
        "    .pip_install(\n",
        "        \"vllm @ git+https://github.com/vllm-project/vllm.git@651c614aa43e497a2e2aab473493ba295201ab20\"\n",
        "    )\n",
        "    .pip_install(\"hf-transfer~=0.1\")\n",
        "    .env({\"HF_HUB_ENABLE_HF_TRANSFER\": \"1\"})\n",
        "    .run_function(download_model_to_folder, secret=Secret.from_name(\"huggingface\"))\n",
        ")\n",
        "\n",
        "stub = Stub(\"vllm\", image=image)\n",
        "\n",
        "\n",
        "# ## The model class\n",
        "#\n",
        "# The inference function is best represented with Modal's [class syntax](/docs/guide/lifecycle-functions) and the `__enter__` method.\n",
        "# This enables us to load the model into memory just once every time a container starts up, and keep it cached\n",
        "# on the GPU for each subsequent invocation of the function.\n",
        "#\n",
        "# The `vLLM` library allows the code to remain quite clean.\n",
        "@stub.cls(gpu=\"A100\", secret=Secret.from_name(\"huggingface\"))\n",
        "class Model:\n",
        "    def __enter__(self):\n",
        "        from vllm import LLM\n",
        "\n",
        "        # Load the model. Tip: MPT models may require `trust_remote_code=true`.\n",
        "        # We also add additional system prompting to the model to help it output json correctly.\n",
        "        self.llm = LLM(MODEL_DIR)\n",
        "        self.template = \"\"\"SYSTEM: Always correctly output response data as correctly formatted json in a codeblock\\n{system}\n",
        "USER: {input}\n",
        "ASSISTANT: ```json\\n\"\"\"\n",
        "\n",
        "    @method()\n",
        "    def generate(\n",
        "        self,\n",
        "        system: str,\n",
        "        inputs: List[str],\n",
        "        max_tokens: int = 800,\n",
        "        temperature: float = 0.1,\n",
        "        presence_penalty: float = 1.15,\n",
        "    ):\n",
        "        from vllm import SamplingParams\n",
        "\n",
        "        prompts = [self.template.format(system=system, input=ii) for ii in inputs]\n",
        "        sampling_params = SamplingParams(\n",
        "            temperature=temperature,\n",
        "            # we add a ``` to the end of the prompt to ensure the model outputs a codeblock\n",
        "            # improving the chances of it outputting correctly formatted json\n",
        "            stop=\"```\",\n",
        "            top_p=1,\n",
        "            max_tokens=max_tokens,\n",
        "            presence_penalty=presence_penalty,\n",
        "        )\n",
        "        result = self.llm.generate(prompts, sampling_params)\n",
        "        num_tokens = 0\n",
        "        results = [output.outputs[0].text for output in result]\n",
        "        num_tokens = sum([len(output.outputs[0].token_ids) for output in result])\n",
        "        return results, num_tokens\n",
        "\n",
        "\n",
        "class InputModel(BaseModel):\n",
        "    system: str\n",
        "    data: List[str]\n",
        "    max_tokens: int = 800\n",
        "    temperature: float = 0.1\n",
        "    presence_penalty: float = 1.15\n",
        "\n",
        "\n",
        "@app.post(\"/\")\n",
        "def main(input: InputModel):\n",
        "    def try_json(x):\n",
        "        try:\n",
        "            return json.loads(x)\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            return x\n",
        "\n",
        "    model = Model()\n",
        "    #data, num_tokens = model.generate.call(\n",
        "    data, num_tokens = model.generate.remote(\n",
        "        system=input.system,\n",
        "        inputs=input.data,\n",
        "        max_tokens=input.max_tokens,\n",
        "        temperature=input.temperature,\n",
        "        presence_penalty=input.presence_penalty,\n",
        "    )\n",
        "    return {\n",
        "        \"data\": [try_json(x) for x in data],\n",
        "        \"num_tokens\": num_tokens,\n",
        "    }\n",
        "\n",
        "\n",
        "@stub.function(image=image)\n",
        "@asgi_app()\n",
        "def fastapi_app():\n",
        "    return app"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwGtQVHTuNrU",
        "outputId": "ac7fbb56-8255-40fb-b37e-e988669f97ee"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing vllm_mistral-7b-api.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run API in Modal"
      ],
      "metadata": {
        "id": "zE1qczj-u0Nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Serve\n",
        "!modal serve vllm_mistral-7b-api.py"
      ],
      "metadata": {
        "id": "Z4nUD5Wuui8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check URL in Browser\n",
        "https://xxx...dev.modal.run/docs\n",
        "\n",
        "### Send POST via Terminal\n",
        "```\n",
        "curl -X 'POST' \\\n",
        "  'https://xxx...dev.modal.run' \\\n",
        "  -H 'accept: application/json' \\\n",
        "  -H 'Content-Type: application/json' \\\n",
        "  -d '{\n",
        "   \"system\":\"Extract Users from `Interface Users { users: Array<{name: string, age:number}>}`\",\n",
        "   \"data\":[\n",
        "      \"James, 33, and Isabella, 23, are among the users with Benjamin, who is 34, Mia, 30, and Ethan, 28.\",\n",
        "      \"Evelyn, 25, and Jacob, 29, are part of the records, along with Abigail, 27, Liam, 32, and Harper, 26.\"\n",
        "   ]\n",
        "}'\n",
        "```"
      ],
      "metadata": {
        "id": "uE4RifBWuzHv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The app - runs in Modal"
      ],
      "metadata": {
        "id": "yV-ta3j-6ZnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# uncomment following line to export\n",
        "# %%writefile vllm_mistral-7b.py\n",
        "\n",
        "#|export\n",
        "\n",
        "# # Fast inference with vLLM (Mistral 7B)\n",
        "#\n",
        "# In this example, we show how to run basic inference, using [`vLLM`](https://github.com/vllm-project/vllm)\n",
        "# to take advantage of PagedAttention, which speeds up sequential inferences with optimized key-value caching.\n",
        "#\n",
        "# `vLLM` also supports a use case as a FastAPI server which we will explore in a future guide. This example\n",
        "# walks through setting up an environment that works with `vLLM ` for basic inference.\n",
        "#\n",
        "# We are running the [Mistral 7B Instruct](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1) model here, which is an instruct fine-tuned version of Mistral's 7B model best fit for conversation.\n",
        "# You can expect 20 second cold starts and well over 100 tokens/second. The larger the batch of prompts, the higher the throughput.\n",
        "# For example, with the 60 prompts below, we can produce 19k tokens in 15 seconds, which is around 1.25k tokens/second.\n",
        "#\n",
        "# To run\n",
        "# [any of the other supported models](https://vllm.readthedocs.io/en/latest/models/supported_models.html),\n",
        "# simply replace the model name in the download step. You may also need to enable `trust_remote_code` for MPT models (see comment below)..\n",
        "#\n",
        "# ## Setup\n",
        "#\n",
        "# First we import the components we need from `modal`.\n",
        "\n",
        "import os\n",
        "\n",
        "from modal import Image, Secret, Stub, method\n",
        "\n",
        "MODEL_DIR = \"/model\"\n",
        "BASE_MODEL = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "\n",
        "# ## Define a container image\n",
        "#\n",
        "# We want to create a Modal image which has the model weights pre-saved to a directory. The benefit of this\n",
        "# is that the container no longer has to re-download the model from Huggingface - instead, it will take\n",
        "# advantage of Modal's internal filesystem for faster cold starts.\n",
        "#\n",
        "# ### Download the weights\n",
        "# Make sure you have created a [HuggingFace access token](https://huggingface.co/settings/tokens).\n",
        "# To access the token in a Modal function, we can create a secret on the [secrets page](https://modal.com/secrets).\n",
        "# Now the token will be available via the environment variable named `HUGGINGFACE_TOKEN`. Functions that inject this secret will have access to the environment variable.\n",
        "#\n",
        "# We can download the model to a particular directory using the HuggingFace utility function `snapshot_download`.\n",
        "#\n",
        "# Tip: avoid using global variables in this function. Changes to code outside this function will not be detected and the download step will not re-run.\n",
        "def download_model_to_folder():\n",
        "    from huggingface_hub import snapshot_download\n",
        "\n",
        "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "    snapshot_download(\n",
        "        BASE_MODEL,\n",
        "        local_dir=MODEL_DIR,\n",
        "        token=os.environ[\"HUGGINGFACE_TOKEN\"],\n",
        "    )\n",
        "\n",
        "\n",
        "# ### Image definition\n",
        "# We’ll start from a Dockerhub image recommended by `vLLM`, upgrade the older\n",
        "# version of `torch` (from 1.14) to a new one specifically built for CUDA 11.8.\n",
        "# Next, we install `vLLM` from source to get the latest updates. Finally, we’ll\n",
        "# use run_function to run the function defined above to ensure the weights of\n",
        "# the model are saved within the container image.\n",
        "image = (\n",
        "    Image.from_registry(\"nvcr.io/nvidia/pytorch:22.12-py3\")\n",
        "    .pip_install(\n",
        "        \"torch==2.0.1+cu118\", index_url=\"https://download.pytorch.org/whl/cu118\"\n",
        "    )\n",
        "    # Pinned to 10/16/23\n",
        "    .pip_install(\n",
        "        \"vllm @ git+https://github.com/vllm-project/vllm.git@651c614aa43e497a2e2aab473493ba295201ab20\"\n",
        "    )\n",
        "    # Use the barebones hf-transfer package for maximum download speeds. No progress bar, but expect 700MB/s.\n",
        "    .pip_install(\"hf-transfer~=0.1\")\n",
        "    .env({\"HF_HUB_ENABLE_HF_TRANSFER\": \"1\"})\n",
        "    .run_function(\n",
        "        download_model_to_folder,\n",
        "        secret=Secret.from_name(\"huggingface\"),\n",
        "        timeout=60 * 20,\n",
        "    )\n",
        ")\n",
        "\n",
        "stub = Stub(\"example-vllm-inference\", image=image)\n",
        "\n",
        "\n",
        "# ## The model class\n",
        "#\n",
        "# The inference function is best represented with Modal's [class syntax](/docs/guide/lifecycle-functions) and the `__enter__` method.\n",
        "# This enables us to load the model into memory just once every time a container starts up, and keep it cached\n",
        "# on the GPU for each subsequent invocation of the function.\n",
        "#\n",
        "# The `vLLM` library allows the code to remain quite clean.\n",
        "@stub.cls(gpu=\"A100\", secret=Secret.from_name(\"huggingface\"))\n",
        "class Model:\n",
        "    def __enter__(self):\n",
        "        from vllm import LLM\n",
        "\n",
        "        # Load the model. Tip: MPT models may require `trust_remote_code=true`.\n",
        "        self.llm = LLM(MODEL_DIR)\n",
        "        self.template = \"\"\"<s>[INST] <<SYS>>\n",
        "{system}\n",
        "<</SYS>>\n",
        "\n",
        "{user} [/INST] \"\"\"\n",
        "\n",
        "    @method()\n",
        "    def generate(self, user_questions):\n",
        "        from vllm import SamplingParams\n",
        "\n",
        "        prompts = [\n",
        "            self.template.format(system=\"\", user=q) for q in user_questions\n",
        "        ]\n",
        "\n",
        "        sampling_params = SamplingParams(\n",
        "            temperature=0.75,\n",
        "            top_p=1,\n",
        "            max_tokens=800,\n",
        "            presence_penalty=1.15,\n",
        "        )\n",
        "        result = self.llm.generate(prompts, sampling_params)\n",
        "        num_tokens = 0\n",
        "        for output in result:\n",
        "            num_tokens += len(output.outputs[0].token_ids)\n",
        "            print(output.prompt, output.outputs[0].text, \"\\n\\n\", sep=\"\")\n",
        "        print(f\"Generated {num_tokens} tokens\")\n",
        "\n",
        "\n",
        "# ## Run the model\n",
        "# We define a [`local_entrypoint`](/docs/guide/apps#entrypoints-for-ephemeral-apps) to call our remote function\n",
        "# sequentially for a list of inputs. You can run this locally with `modal run vllm_inference.py`.\n",
        "@stub.local_entrypoint()\n",
        "def main():\n",
        "    model = Model()\n",
        "    questions = [\n",
        "        # Coding questions\n",
        "        \"Implement a Python function to compute the Fibonacci numbers.\",\n",
        "        \"Write a Rust function that performs binary exponentiation.\",\n",
        "        \"How do I allocate memory in C?\",\n",
        "        \"What are the differences between Javascript and Python?\",\n",
        "        \"How do I find invalid indices in Postgres?\",\n",
        "        \"How can you implement a LRU (Least Recently Used) cache in Python?\",\n",
        "        \"What approach would you use to detect and prevent race conditions in a multithreaded application?\",\n",
        "        \"Can you explain how a decision tree algorithm works in machine learning?\",\n",
        "        \"How would you design a simple key-value store database from scratch?\",\n",
        "        \"How do you handle deadlock situations in concurrent programming?\",\n",
        "        \"What is the logic behind the A* search algorithm, and where is it used?\",\n",
        "        \"How can you design an efficient autocomplete system?\",\n",
        "        \"What approach would you take to design a secure session management system in a web application?\",\n",
        "        \"How would you handle collision in a hash table?\",\n",
        "        \"How can you implement a load balancer for a distributed system?\",\n",
        "        # Literature\n",
        "        \"What is the fable involving a fox and grapes?\",\n",
        "        \"Write a story in the style of James Joyce about a trip to the Australian outback in 2083, to see robots in the beautiful desert.\",\n",
        "        \"Who does Harry turn into a balloon?\",\n",
        "        \"Write a tale about a time-traveling historian who's determined to witness the most significant events in human history.\",\n",
        "        \"Describe a day in the life of a secret agent who's also a full-time parent.\",\n",
        "        \"Create a story about a detective who can communicate with animals.\",\n",
        "        \"What is the most unusual thing about living in a city floating in the clouds?\",\n",
        "        \"In a world where dreams are shared, what happens when a nightmare invades a peaceful dream?\",\n",
        "        \"Describe the adventure of a lifetime for a group of friends who found a map leading to a parallel universe.\",\n",
        "        \"Tell a story about a musician who discovers that their music has magical powers.\",\n",
        "        \"In a world where people age backwards, describe the life of a 5-year-old man.\",\n",
        "        \"Create a tale about a painter whose artwork comes to life every night.\",\n",
        "        \"What happens when a poet's verses start to predict future events?\",\n",
        "        \"Imagine a world where books can talk. How does a librarian handle them?\",\n",
        "        \"Tell a story about an astronaut who discovered a planet populated by plants.\",\n",
        "        \"Describe the journey of a letter traveling through the most sophisticated postal service ever.\",\n",
        "        \"Write a tale about a chef whose food can evoke memories from the eater's past.\",\n",
        "        # History\n",
        "        \"What were the major contributing factors to the fall of the Roman Empire?\",\n",
        "        \"How did the invention of the printing press revolutionize European society?\",\n",
        "        \"What are the effects of quantitative easing?\",\n",
        "        \"How did the Greek philosophers influence economic thought in the ancient world?\",\n",
        "        \"What were the economic and philosophical factors that led to the fall of the Soviet Union?\",\n",
        "        \"How did decolonization in the 20th century change the geopolitical map?\",\n",
        "        \"What was the influence of the Khmer Empire on Southeast Asia's history and culture?\",\n",
        "        # Thoughtfulness\n",
        "        \"Describe the city of the future, considering advances in technology, environmental changes, and societal shifts.\",\n",
        "        \"In a dystopian future where water is the most valuable commodity, how would society function?\",\n",
        "        \"If a scientist discovers immortality, how could this impact society, economy, and the environment?\",\n",
        "        \"What could be the potential implications of contact with an advanced alien civilization?\",\n",
        "        # Math\n",
        "        \"What is the product of 9 and 8?\",\n",
        "        \"If a train travels 120 kilometers in 2 hours, what is its average speed?\",\n",
        "        \"Think through this step by step. If the sequence a_n is defined by a_1 = 3, a_2 = 5, and a_n = a_(n-1) + a_(n-2) for n > 2, find a_6.\",\n",
        "        \"Think through this step by step. Calculate the sum of an arithmetic series with first term 3, last term 35, and total terms 11.\",\n",
        "        \"Think through this step by step. What is the area of a triangle with vertices at the points (1,2), (3,-4), and (-2,5)?\",\n",
        "        \"Think through this step by step. Solve the following system of linear equations: 3x + 2y = 14, 5x - y = 15.\",\n",
        "        # Facts\n",
        "        \"Who was Emperor Norton I, and what was his significance in San Francisco's history?\",\n",
        "        \"What is the Voynich manuscript, and why has it perplexed scholars for centuries?\",\n",
        "        \"What was Project A119 and what were its objectives?\",\n",
        "        \"What is the 'Dyatlov Pass incident' and why does it remain a mystery?\",\n",
        "        \"What is the 'Emu War' that took place in Australia in the 1930s?\",\n",
        "        \"What is the 'Phantom Time Hypothesis' proposed by Heribert Illig?\",\n",
        "        \"Who was the 'Green Children of Woolpit' as per 12th-century English legend?\",\n",
        "        \"What are 'zombie stars' in the context of astronomy?\",\n",
        "        \"Who were the 'Dog-Headed Saint' and the 'Lion-Faced Saint' in medieval Christian traditions?\",\n",
        "        \"What is the story of the 'Globsters', unidentified organic masses washed up on the shores?\",\n",
        "    ]\n",
        "    model.generate.remote(questions)"
      ],
      "metadata": {
        "id": "rDsRlP7m1-um",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87fd6091-9be3-43e3-d423-4b63ede51e23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing vllm_mistral-7b.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run app in Modal"
      ],
      "metadata": {
        "id": "nKaBR3-_7qWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Local run\n",
        "# 1st run / initial run: 14 mins.\n",
        "# 2nd run: 1 min.\n",
        "# 3rd run (12 hours later): 1 min.\n",
        "!modal run vllm_mistral-7b.py"
      ],
      "metadata": {
        "id": "4d-Kxt51IJXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Export production file for Modal with nbdev\n",
        "\n",
        "The notebook file will be copied from **Google drive** to the current notebook environment.\n",
        "\n",
        "Alternative?: %notebook modal_hello_world2.ipynb"
      ],
      "metadata": {
        "id": "NRQoSKU51v6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect Google Drive\n",
        "# This and the following cell can be skipped it the notebook file is copied manually to the current notebook environment\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "RI3oYbrHwxtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy Notebook to local path\n",
        "!cp \"/content/drive/MyDrive/Colab Notebooks/vllm_mistral-7b.ipynb\" /content/"
      ],
      "metadata": {
        "id": "KF3b5UaVyi8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export source code marked with #|export\n",
        "from nbdev.export import nb_export\n",
        "nb_export('/content/BLOOM_176B_Model.ipynb', lib_path='.', name='vllm_mistral-7b')"
      ],
      "metadata": {
        "id": "TqPqsbyv7X5Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}