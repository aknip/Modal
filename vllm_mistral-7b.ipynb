{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1GR4CBhwEVfzZHIOk6SZsCP6vFs0yjYWp",
      "authorship_tag": "ABX9TyMOhAGpyuORgv3ydaZsNIx1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aknip/Modal/blob/main/vllm_mistral-7b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VLLM Mistral 7B Model\n",
        "\n",
        "The app then can be automatically deployed to Modal.com.\n",
        "- Export Notebook to .py-file automatically (via nbdev)\n",
        "- Serve or deploy .py-file to Modal.com automatically\n",
        "\n",
        "Code blocks wich are needed for the final .py-file (for Modal.com) are marked with `#|export`\n",
        "\n",
        "Sources:  \n",
        "- https://github.com/modal-labs/modal-examples/blob/main/06_gpu_and_ml/vllm_inference.py"
      ],
      "metadata": {
        "id": "u7aFXzti5Okh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "IN_NOTEBOOK = any([\"jupyter-notebook\" in i for i in psutil.Process().parent().cmdline()])"
      ],
      "metadata": {
        "id": "HBQ9fzwbCTrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzJ0fadPQW5V"
      },
      "outputs": [],
      "source": [
        "!pip install modal nbdev -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from getpass import getpass\n",
        "if IN_NOTEBOOK:\n",
        "  CREDS = json.loads(getpass(\"Secrets (JSON string): \"))\n",
        "  os.environ['CREDS'] = json.dumps(CREDS)\n",
        "  CREDS = json.loads(os.getenv('CREDS'))"
      ],
      "metadata": {
        "id": "D94_wG68CccK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"MODAL_TOKEN_ID\"] = CREDS['MODAL']['MODAL_TOKEN_ID']['credential']\n",
        "os.environ[\"MODAL_TOKEN_SECRET\"] = CREDS['MODAL']['MODAL_TOKEN_SECRET']['credential']"
      ],
      "metadata": {
        "id": "E9abYug6TRcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The app - runs in Modal"
      ],
      "metadata": {
        "id": "yV-ta3j-6ZnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# uncomment following line to export\n",
        "# %%writefile vllm_mistral-7b.py\n",
        "\n",
        "#|export\n",
        "\n",
        "# # Fast inference with vLLM (Mistral 7B)\n",
        "#\n",
        "# In this example, we show how to run basic inference, using [`vLLM`](https://github.com/vllm-project/vllm)\n",
        "# to take advantage of PagedAttention, which speeds up sequential inferences with optimized key-value caching.\n",
        "#\n",
        "# `vLLM` also supports a use case as a FastAPI server which we will explore in a future guide. This example\n",
        "# walks through setting up an environment that works with `vLLM ` for basic inference.\n",
        "#\n",
        "# We are running the [Mistral 7B Instruct](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1) model here, which is an instruct fine-tuned version of Mistral's 7B model best fit for conversation.\n",
        "# You can expect 20 second cold starts and well over 100 tokens/second. The larger the batch of prompts, the higher the throughput.\n",
        "# For example, with the 60 prompts below, we can produce 19k tokens in 15 seconds, which is around 1.25k tokens/second.\n",
        "#\n",
        "# To run\n",
        "# [any of the other supported models](https://vllm.readthedocs.io/en/latest/models/supported_models.html),\n",
        "# simply replace the model name in the download step. You may also need to enable `trust_remote_code` for MPT models (see comment below)..\n",
        "#\n",
        "# ## Setup\n",
        "#\n",
        "# First we import the components we need from `modal`.\n",
        "\n",
        "import os\n",
        "\n",
        "from modal import Image, Secret, Stub, method\n",
        "\n",
        "MODEL_DIR = \"/model\"\n",
        "BASE_MODEL = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "\n",
        "# ## Define a container image\n",
        "#\n",
        "# We want to create a Modal image which has the model weights pre-saved to a directory. The benefit of this\n",
        "# is that the container no longer has to re-download the model from Huggingface - instead, it will take\n",
        "# advantage of Modal's internal filesystem for faster cold starts.\n",
        "#\n",
        "# ### Download the weights\n",
        "# Make sure you have created a [HuggingFace access token](https://huggingface.co/settings/tokens).\n",
        "# To access the token in a Modal function, we can create a secret on the [secrets page](https://modal.com/secrets).\n",
        "# Now the token will be available via the environment variable named `HUGGINGFACE_TOKEN`. Functions that inject this secret will have access to the environment variable.\n",
        "#\n",
        "# We can download the model to a particular directory using the HuggingFace utility function `snapshot_download`.\n",
        "#\n",
        "# Tip: avoid using global variables in this function. Changes to code outside this function will not be detected and the download step will not re-run.\n",
        "def download_model_to_folder():\n",
        "    from huggingface_hub import snapshot_download\n",
        "\n",
        "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "    snapshot_download(\n",
        "        BASE_MODEL,\n",
        "        local_dir=MODEL_DIR,\n",
        "        token=os.environ[\"HUGGINGFACE_TOKEN\"],\n",
        "    )\n",
        "\n",
        "\n",
        "# ### Image definition\n",
        "# We’ll start from a Dockerhub image recommended by `vLLM`, upgrade the older\n",
        "# version of `torch` (from 1.14) to a new one specifically built for CUDA 11.8.\n",
        "# Next, we install `vLLM` from source to get the latest updates. Finally, we’ll\n",
        "# use run_function to run the function defined above to ensure the weights of\n",
        "# the model are saved within the container image.\n",
        "image = (\n",
        "    Image.from_registry(\"nvcr.io/nvidia/pytorch:22.12-py3\")\n",
        "    .pip_install(\n",
        "        \"torch==2.0.1+cu118\", index_url=\"https://download.pytorch.org/whl/cu118\"\n",
        "    )\n",
        "    # Pinned to 10/16/23\n",
        "    .pip_install(\n",
        "        \"vllm @ git+https://github.com/vllm-project/vllm.git@651c614aa43e497a2e2aab473493ba295201ab20\"\n",
        "    )\n",
        "    # Use the barebones hf-transfer package for maximum download speeds. No progress bar, but expect 700MB/s.\n",
        "    .pip_install(\"hf-transfer~=0.1\")\n",
        "    .env({\"HF_HUB_ENABLE_HF_TRANSFER\": \"1\"})\n",
        "    .run_function(\n",
        "        download_model_to_folder,\n",
        "        secret=Secret.from_name(\"huggingface\"),\n",
        "        timeout=60 * 20,\n",
        "    )\n",
        ")\n",
        "\n",
        "stub = Stub(\"example-vllm-inference\", image=image)\n",
        "\n",
        "\n",
        "# ## The model class\n",
        "#\n",
        "# The inference function is best represented with Modal's [class syntax](/docs/guide/lifecycle-functions) and the `__enter__` method.\n",
        "# This enables us to load the model into memory just once every time a container starts up, and keep it cached\n",
        "# on the GPU for each subsequent invocation of the function.\n",
        "#\n",
        "# The `vLLM` library allows the code to remain quite clean.\n",
        "@stub.cls(gpu=\"A100\", secret=Secret.from_name(\"huggingface\"))\n",
        "class Model:\n",
        "    def __enter__(self):\n",
        "        from vllm import LLM\n",
        "\n",
        "        # Load the model. Tip: MPT models may require `trust_remote_code=true`.\n",
        "        self.llm = LLM(MODEL_DIR)\n",
        "        self.template = \"\"\"<s>[INST] <<SYS>>\n",
        "{system}\n",
        "<</SYS>>\n",
        "\n",
        "{user} [/INST] \"\"\"\n",
        "\n",
        "    @method()\n",
        "    def generate(self, user_questions):\n",
        "        from vllm import SamplingParams\n",
        "\n",
        "        prompts = [\n",
        "            self.template.format(system=\"\", user=q) for q in user_questions\n",
        "        ]\n",
        "\n",
        "        sampling_params = SamplingParams(\n",
        "            temperature=0.75,\n",
        "            top_p=1,\n",
        "            max_tokens=800,\n",
        "            presence_penalty=1.15,\n",
        "        )\n",
        "        result = self.llm.generate(prompts, sampling_params)\n",
        "        num_tokens = 0\n",
        "        for output in result:\n",
        "            num_tokens += len(output.outputs[0].token_ids)\n",
        "            print(output.prompt, output.outputs[0].text, \"\\n\\n\", sep=\"\")\n",
        "        print(f\"Generated {num_tokens} tokens\")\n",
        "\n",
        "\n",
        "# ## Run the model\n",
        "# We define a [`local_entrypoint`](/docs/guide/apps#entrypoints-for-ephemeral-apps) to call our remote function\n",
        "# sequentially for a list of inputs. You can run this locally with `modal run vllm_inference.py`.\n",
        "@stub.local_entrypoint()\n",
        "def main():\n",
        "    model = Model()\n",
        "    questions = [\n",
        "        # Coding questions\n",
        "        \"Implement a Python function to compute the Fibonacci numbers.\",\n",
        "        \"Write a Rust function that performs binary exponentiation.\",\n",
        "        \"How do I allocate memory in C?\",\n",
        "        \"What are the differences between Javascript and Python?\",\n",
        "        \"How do I find invalid indices in Postgres?\",\n",
        "        \"How can you implement a LRU (Least Recently Used) cache in Python?\",\n",
        "        \"What approach would you use to detect and prevent race conditions in a multithreaded application?\",\n",
        "        \"Can you explain how a decision tree algorithm works in machine learning?\",\n",
        "        \"How would you design a simple key-value store database from scratch?\",\n",
        "        \"How do you handle deadlock situations in concurrent programming?\",\n",
        "        \"What is the logic behind the A* search algorithm, and where is it used?\",\n",
        "        \"How can you design an efficient autocomplete system?\",\n",
        "        \"What approach would you take to design a secure session management system in a web application?\",\n",
        "        \"How would you handle collision in a hash table?\",\n",
        "        \"How can you implement a load balancer for a distributed system?\",\n",
        "        # Literature\n",
        "        \"What is the fable involving a fox and grapes?\",\n",
        "        \"Write a story in the style of James Joyce about a trip to the Australian outback in 2083, to see robots in the beautiful desert.\",\n",
        "        \"Who does Harry turn into a balloon?\",\n",
        "        \"Write a tale about a time-traveling historian who's determined to witness the most significant events in human history.\",\n",
        "        \"Describe a day in the life of a secret agent who's also a full-time parent.\",\n",
        "        \"Create a story about a detective who can communicate with animals.\",\n",
        "        \"What is the most unusual thing about living in a city floating in the clouds?\",\n",
        "        \"In a world where dreams are shared, what happens when a nightmare invades a peaceful dream?\",\n",
        "        \"Describe the adventure of a lifetime for a group of friends who found a map leading to a parallel universe.\",\n",
        "        \"Tell a story about a musician who discovers that their music has magical powers.\",\n",
        "        \"In a world where people age backwards, describe the life of a 5-year-old man.\",\n",
        "        \"Create a tale about a painter whose artwork comes to life every night.\",\n",
        "        \"What happens when a poet's verses start to predict future events?\",\n",
        "        \"Imagine a world where books can talk. How does a librarian handle them?\",\n",
        "        \"Tell a story about an astronaut who discovered a planet populated by plants.\",\n",
        "        \"Describe the journey of a letter traveling through the most sophisticated postal service ever.\",\n",
        "        \"Write a tale about a chef whose food can evoke memories from the eater's past.\",\n",
        "        # History\n",
        "        \"What were the major contributing factors to the fall of the Roman Empire?\",\n",
        "        \"How did the invention of the printing press revolutionize European society?\",\n",
        "        \"What are the effects of quantitative easing?\",\n",
        "        \"How did the Greek philosophers influence economic thought in the ancient world?\",\n",
        "        \"What were the economic and philosophical factors that led to the fall of the Soviet Union?\",\n",
        "        \"How did decolonization in the 20th century change the geopolitical map?\",\n",
        "        \"What was the influence of the Khmer Empire on Southeast Asia's history and culture?\",\n",
        "        # Thoughtfulness\n",
        "        \"Describe the city of the future, considering advances in technology, environmental changes, and societal shifts.\",\n",
        "        \"In a dystopian future where water is the most valuable commodity, how would society function?\",\n",
        "        \"If a scientist discovers immortality, how could this impact society, economy, and the environment?\",\n",
        "        \"What could be the potential implications of contact with an advanced alien civilization?\",\n",
        "        # Math\n",
        "        \"What is the product of 9 and 8?\",\n",
        "        \"If a train travels 120 kilometers in 2 hours, what is its average speed?\",\n",
        "        \"Think through this step by step. If the sequence a_n is defined by a_1 = 3, a_2 = 5, and a_n = a_(n-1) + a_(n-2) for n > 2, find a_6.\",\n",
        "        \"Think through this step by step. Calculate the sum of an arithmetic series with first term 3, last term 35, and total terms 11.\",\n",
        "        \"Think through this step by step. What is the area of a triangle with vertices at the points (1,2), (3,-4), and (-2,5)?\",\n",
        "        \"Think through this step by step. Solve the following system of linear equations: 3x + 2y = 14, 5x - y = 15.\",\n",
        "        # Facts\n",
        "        \"Who was Emperor Norton I, and what was his significance in San Francisco's history?\",\n",
        "        \"What is the Voynich manuscript, and why has it perplexed scholars for centuries?\",\n",
        "        \"What was Project A119 and what were its objectives?\",\n",
        "        \"What is the 'Dyatlov Pass incident' and why does it remain a mystery?\",\n",
        "        \"What is the 'Emu War' that took place in Australia in the 1930s?\",\n",
        "        \"What is the 'Phantom Time Hypothesis' proposed by Heribert Illig?\",\n",
        "        \"Who was the 'Green Children of Woolpit' as per 12th-century English legend?\",\n",
        "        \"What are 'zombie stars' in the context of astronomy?\",\n",
        "        \"Who were the 'Dog-Headed Saint' and the 'Lion-Faced Saint' in medieval Christian traditions?\",\n",
        "        \"What is the story of the 'Globsters', unidentified organic masses washed up on the shores?\",\n",
        "    ]\n",
        "    model.generate.remote(questions)"
      ],
      "metadata": {
        "id": "rDsRlP7m1-um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Export production file for Modal with nbdev\n",
        "\n",
        "The notebook file will be copied from **Google drive** to the current notebook environment.\n",
        "\n",
        "Alternative?: %notebook modal_hello_world2.ipynb"
      ],
      "metadata": {
        "id": "NRQoSKU51v6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect Google Drive\n",
        "# This and the following cell can be skipped it the notebook file is copied manually to the current notebook environment\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "RI3oYbrHwxtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy Notebook to local path\n",
        "!cp \"/content/drive/MyDrive/Colab Notebooks/vllm_mistral-7b.ipynb\" /content/"
      ],
      "metadata": {
        "id": "KF3b5UaVyi8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export source code marked with #|export\n",
        "from nbdev.export import nb_export\n",
        "nb_export('/content/BLOOM_176B_Model.ipynb', lib_path='.', name='vllm_mistral-7b')"
      ],
      "metadata": {
        "id": "TqPqsbyv7X5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run in Modal"
      ],
      "metadata": {
        "id": "nKaBR3-_7qWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dev\n",
        "!modal serve vllm_mistral-7b.py"
      ],
      "metadata": {
        "id": "1X3slh4n3xGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Deploy server permanently\n",
        "!modal deploy vllm_mistral-7b.py"
      ],
      "metadata": {
        "id": "fn2ufbIe47vc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}